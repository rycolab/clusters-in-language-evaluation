{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ef892",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 510\n",
    "DEFAULT_NUM_EXAMPLES = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import mauve \n",
    "import copy\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c93a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from util import util\n",
    "from h01_data.get_clusters import get_clusters\n",
    "from utils import clean, load_gpt2_dataset, get_representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25cd7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dce91c",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd3f94",
   "metadata": {},
   "source": [
    "### WebText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ffc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_text = load_gpt2_dataset('data/amazon.valid.jsonl', num_examples=DEFAULT_NUM_EXAMPLES) # human\n",
    "p_text2 = load_gpt2_dataset('data/amazon.valid.jsonl', num_examples=DEFAULT_NUM_EXAMPLES*2)[DEFAULT_NUM_EXAMPLES:]\n",
    "q_text = load_gpt2_dataset('data/amazon-xl-1542M.valid.jsonl', num_examples=DEFAULT_NUM_EXAMPLES)\n",
    "q_text2 = load_gpt2_dataset('data/amazon-xl-1542M.valid.jsonl', num_examples=DEFAULT_NUM_EXAMPLES*2)[DEFAULT_NUM_EXAMPLES:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92fad5",
   "metadata": {},
   "source": [
    "### Yelp Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 10000\n",
    "num_test = 5000\n",
    "sent_dataset = load_dataset('yelp_polarity', split='train').shuffle(seed=0)[:num_train]\n",
    "sent_dataset_test = load_dataset('yelp_polarity', split='test').shuffle(seed=0)[:num_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text, sent_labels = zip(*[(t.replace('\\n', ' '), l) for t, l in zip(sent_dataset['text'], sent_dataset['label']) if t])\n",
    "sent_test_text, sent_test_labels = zip(*[(t.replace('\\n', ' '), l) for t, l in zip(sent_dataset_test['text'], sent_dataset_test['label']) if t])\n",
    "\n",
    "sent_p_text = sent_text[:num_train//2]\n",
    "sent_p_text2 = sent_text[num_train//2:]\n",
    "sent_test_p_text = sent_test_text[:num_test//2]\n",
    "sent_test_p_text2 =  sent_test_text[num_test//2:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b83ac",
   "metadata": {},
   "source": [
    "### News Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ec66e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "author_dataset = load_dataset('Fraser/news-category-dataset', split='train')\n",
    "author_dataset_test = load_dataset('Fraser/news-category-dataset', split='test')\n",
    "author_labels, author_labels_test  = [x.split(\",\")[0] for x in author_dataset['authors']],  [x.split(\",\")[0] for x in author_dataset_test['authors']]\n",
    "counts = sorted(Counter(author_labels).items(), key=lambda item: item[1], reverse=True)\n",
    "authors = set([a for a, n in counts if n >=400])\n",
    "\n",
    "\n",
    "author_labels_filtered = [a for a in author_labels if (a and a in authors)]\n",
    "author_labels_test_filtered = [a for a in author_labels_test if (a and a in authors)]\n",
    "\n",
    "with open('articles_full.txt') as f:\n",
    "    articles = f.read().splitlines()\n",
    "    \n",
    "with open('articles_full_test.txt') as f:\n",
    "    articles_test = f.read().splitlines()\n",
    "    \n",
    "assert len(articles) == len(author_labels_filtered)\n",
    "assert len(articles_test) == len(author_labels_test_filtered)\n",
    "\n",
    "articles, author_labels_filtered = zip(*[(a,b) for a,b in zip(articles, author_labels_filtered) if a])\n",
    "articles_test, author_labels_test_filtered = zip(*[(a,b) for a,b in zip(articles_test, author_labels_test_filtered) if a])\n",
    "\n",
    "num = len(articles)//2\n",
    "authors_p_text = articles[:num]\n",
    "authors_p_text2 = articles[num:]\n",
    "\n",
    "num = len(articles_test)//2\n",
    "authors_test_p_text = articles_test[:num]\n",
    "authors_test_p_text2 = articles_test[num:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d1bcc2",
   "metadata": {},
   "source": [
    "### 20 NewsGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cbe383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4789d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(newsgroups_train.data)//2\n",
    "data, labels = zip(*[(t.replace('\\n', ' '), l) for t, l in zip(newsgroups_train['data'], newsgroups_train['target']) if t])\n",
    "news_p_text = data[:num]\n",
    "news_p_text2 = data[num:num*2]\n",
    "test_news_data, test_news_labels = zip(*[(t.replace('\\n', ' '), l) for t, l in zip(newsgroups_test['data'], newsgroups_test['target']) if t])\n",
    "test_news_p_text = test_news_data[:len(test_news_data)//2]\n",
    "test_news_p_text2 =  test_news_data[len(test_news_data)//2:]\n",
    "\n",
    "news_labels = labels[:num*2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec53977c",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1e9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "from rpy2.robjects import r, pandas2ri\n",
    "pandas2ri.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671db2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(lme4)\n",
    "library(ggplot2)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e0c76",
   "metadata": {},
   "source": [
    "### Base Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = ['the', 'a', 'an']\n",
    "mapping = {'the':'a', 'a':'the', 'an':'the','The':'A', 'A':'The', 'An':'The'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb566fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_text_trunc = tuple([' '.join(i.split()[0:MAX_LEN]) for i in p_text])\n",
    "q_text_trunc = tuple([' '.join(i.split()[0:MAX_LEN]) for i in q_text])\n",
    "p_text2_trunc = tuple([' '.join(i.split()[0:MAX_LEN]) for i in p_text2])\n",
    "base_text = p_text2_trunc\n",
    "p_text2_short = tuple([' '.join(i.split()[0:MAX_LEN//3]) for i in base_text])\n",
    "p_text2_remove = tuple([' '.join([i for i in string.split() if i not in articles]) for string in base_text])\n",
    "temp = [i.split()for i in base_text]\n",
    "for i in temp:\n",
    "    last = i.pop()\n",
    "    random.Random(0).shuffle(i)\n",
    "    i.append(last)\n",
    "p_text2_rand = tuple([' '.join(string) for string in temp])\n",
    "p_text2_nostop = tuple([' '.join([w for w in string.split() if w.lower() not in stops]) for string in base_text])\n",
    "sentences = [sent_tokenize(t) for t in base_text]\n",
    "new_inds = list(range(len(sentences)))\n",
    "random.Random(0).shuffle(new_inds)\n",
    "p_text2_sent_swap = []\n",
    "for i,j in enumerate(new_inds):\n",
    "    p_text2_sent_swap.append(' '.join(sentences[i][:len(sentences[i])//2] + sentences[j][len(sentences[j])//2:]))\n",
    "p_text2_sent_swap = tuple(p_text2_sent_swap)\n",
    "text_names = {p_text_trunc: 'p_text', p_text2_trunc: 'p_text2', q_text_trunc:'q_text',\n",
    "              p_text2_remove: 'p_text2_remove',  p_text2_rand:'p_text2_rand', p_text2_nostop:'p_text2_nostop',\n",
    "             p_text2_short:'p_text2_short',p_text2_sent_swap: 'p_text2_sent_swap'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae274d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_texts = [p_text2_trunc, p_text2_remove, p_text2_rand, p_text2_nostop, q_text_trunc, p_text2_sent_swap, p_text2_short]\n",
    "default_args = {'p_text':p_text_trunc, 'q_text':p_text2_trunc, 'featurize_model_name':'gpt2','kmeans_num_redo':1, \n",
    "                          'kmeans_max_iter':2, 'kmeans_explained_var':0.9, 'is_mean':False, \n",
    "                          'device_id':0, 'max_text_length': MAX_LEN, 'verbose':True, 'laplace': 1}\n",
    "args = {'featurize_model_name': ['bert-base-cased', 'bert-large-cased','gpt2','gpt2-medium','gpt2-large','gpt2-xl'],\n",
    "        'q_text': q_texts, \n",
    "        'is_mean':[True, False],\n",
    "        'num_buckets':[10, 20, 50, 100, 250, 500], \n",
    "        'laplace': [0, 1]}\n",
    "\n",
    "def param_search(args, default_args):\n",
    "    def func(cur_args):\n",
    "        params = copy.deepcopy(default_args)\n",
    "        params.update(cur_args)\n",
    "        p_text, q_text = params.pop('p_text'), params.pop('q_text')\n",
    "        model_name, mean = params['featurize_model_name'], params['is_mean']\n",
    "        \n",
    "        params['p_features'] = get_representations(p_text, model_name, mean)\n",
    "        params['q_features'] = get_representations(q_text, model_name, mean)\n",
    "        return mauve.compute_mauve(**params)\n",
    "    keys = args.keys()\n",
    "    vals = args.values()\n",
    "    data = []\n",
    "    for comb in list(itertools.product(*vals)):\n",
    "        temp = {k:v for k,v in zip(keys,comb)}\n",
    "        m = func(temp)\n",
    "        for val in ['mauve', 'forward_kl', 'backward_kl','exponentiated_kl','js']:\n",
    "            temp[val] = getattr(m, val)\n",
    "        temp['q_text'] = text_names[temp['q_text']]\n",
    "        data.append(temp)\n",
    "    return pd.DataFrame(data)\n",
    "base_df = param_search(args,default_args)  \n",
    "base_df['auc'] = 1 - base_df['mauve']\n",
    "base_df['q_text'] = base_df.q_text.astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b759d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_vars=['auc', 'forward_kl', 'backward_kl','exponentiated_kl','js']\n",
    "id_vars = set(base_df.columns) - set(value_vars)\n",
    "base_df_long = pd.melt(base_df, value_vars=value_vars, id_vars=id_vars, var_name=\"metric\")\n",
    "base_df_long.loc[:, \"metric\"] = base_df_long[\"metric\"] +  base_df_long[\"laplace\"].astype(str)\n",
    "%R -i base_df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3492d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "legend_measure <- 'auc0'\n",
    "tmp <- filter(base_df_long, metric==legend_measure, num_buckets==500, is_mean, featurize_model_name==\"gpt2-xl\")\n",
    "levels <- tmp[order(-tmp$value),]$q_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "metrics <- c('forward_kl1', 'backward_kl1','exponentiated_kl1','js0','auc0' )\n",
    "names(metrics) <- c('Forward', 'Backward','Exponentiated','JS','AUC' )\n",
    "df_subset <- filter(base_df_long, metric %in% metrics, is_mean, featurize_model_name %in% c(\"gpt2-xl\", 'bert-large-cased'))#, q_text != 'p_text2_rand', q_text != 'p_text2_nostop')\n",
    "supp.labs <- c(\"gpt2\",  \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\",\"bert-base-cased\", 'bert-large-cased')\n",
    "names(supp.labs) <- c(\"GPT-2 Small\",  \"GPT-2 Medium\", \"GPT-2 Large\",\"GPT-2 XL\", \"BERT Base\", \"BERT Large\")\n",
    "dist_labels = c(p_text2=expression(paste(italic(\"p\"))), p_text2_remove=expression(paste(italic(\"p\")[\"no art\"])), \n",
    "                                  p_text2_rand=expression(paste(italic(\"p\")[\"rand\"])), p_text2_nostop=expression(paste(italic(\"p\")[\"no stop\"])), \n",
    "                                  q_text=expression(paste(italic(\"q\"))), p_text2_sent_swap=expression(paste(italic(\"p\")[\"swap\"])),\n",
    "                                  p_text2_short=expression(paste(italic(\"p\")[\"short\"]))\n",
    "                             )\n",
    "dist_labels_2 <- c(p_text2=expression(paste(italic(\"p\")^(2))), p_text2_remove=expression(paste(italic(\"p\")[\"no art\"]^(1))), \n",
    "                                  p_text2_rand=expression(paste(italic(\"p\")[\"rand\"]^(1))), p_text2_nostop=expression(paste(italic(\"p\")[\"no stop\"]^(1))), \n",
    "                                  q_text=expression(paste(italic(\"q\"))), p_text2_sent_swap=expression(paste(italic(\"p\")[\"swap\"]^(1))),\n",
    "                                  p_text2_short=expression(paste(italic(\"p\")[\"short\"]^(1)))\n",
    "                             )\n",
    "ggplot(aes(x = num_buckets, y = value, \n",
    "           color=factor(q_text, levels),\n",
    "           shape=factor(q_text, levels)), data = df_subset) +\n",
    "    geom_point(size=3) +\n",
    "    geom_line() +\n",
    "    labs(x = \"Number of Clusters\", y= expression(Delta)) +\n",
    "    scale_color_discrete(name='Comparison\\n      Text', labels=dist_labels) +\n",
    "    scale_shape_manual(name='Comparison\\n      Text', labels=dist_labels, values=sample(seq(15,21))) +\n",
    "    facet_wrap(factor(featurize_model_name, levels=supp.labs, labels=names(supp.labs))~factor(metric, levels=metrics, names(metrics)), scales=\"free_y\", ncol=5) +\n",
    "    scale_x_continuous(trans='log2', breaks=c(10, 20, 50, 100, 250, 500)) +\n",
    "    theme_bw() +\n",
    "    theme(text=element_text(size=13,family=\"serif\"), \n",
    "         axis.text.y=element_text(size=10,family=\"serif\"),\n",
    "         axis.title.y=element_text(size=17,family=\"serif\"),\n",
    "         legend.title=element_text(size=13,family=\"serif\"),\n",
    "         axis.text.x=element_text(size=10,family=\"serif\", angle=30),\n",
    "         aspect.ratio=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcef70f",
   "metadata": {},
   "source": [
    "### Classifying Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84709850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_text1, test_text2 = sent_test_p_text, sent_test_p_text2\n",
    "train_text1, train_text2 = sent_p_text, sent_p_text2\n",
    "trn_labels, tst_labels = sent_labels, sent_test_labels\n",
    "test_text1, test_text2 = test_news_p_text, test_news_p_text2\n",
    "train_text1, train_text2 = news_p_text, news_p_text2\n",
    "trn_labels, tst_labels = news_labels, test_news_labels\n",
    "\n",
    "default_args = {'test_text': test_text1 + test_text2, 'train_text': (train_text1, train_text2),\n",
    "                'cluster_text': (tuple(p_text), tuple(q_text)), \n",
    "                'featurize_model_name':'gpt2','kmeans_num_redo':5, \n",
    "                'kmeans_max_iter':100, 'kmeans_explained_var':0.9, 'is_mean':False, \n",
    "                'device_id':0, 'max_text_length': MAX_LEN, 'verbose':True}\n",
    "args = {'featurize_model_name':[\"bert-base-cased\", 'bert-large-cased',\"gpt2\",  \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"],\n",
    "        'kmeans_explained_var':[0.9], \n",
    "        'cluster_text': [(train_text1, train_text2), (tuple(p_text), tuple(q_text))],\n",
    "        'is_mean': [True, False],\n",
    "        'num_buckets':[len(set(trn_labels)), len(set(trn_labels))*2, 10, 25, 50, 100, 250, 500]}\n",
    "\n",
    "\n",
    "def param_search(args, default_args, train_labels, test_labels):\n",
    "    def func(cur_args):\n",
    "        params = copy.deepcopy(default_args)\n",
    "        params.update(cur_args)\n",
    "        test_text = params['test_text']\n",
    "        p_train_text, q_train_text = params['train_text']\n",
    "        p_cluster_text, q_cluster_text = params['cluster_text']\n",
    "        model_name, mean = params['featurize_model_name'], params['is_mean']\n",
    "        \n",
    "        test_representations = get_representations(test_text, model_name, mean)\n",
    "        train_representations = np.concatenate([get_representations(p_train_text, model_name, mean), \n",
    "                                                get_representations(q_train_text, model_name, mean)], axis=0)\n",
    "        \n",
    "        p_cluster_representations = get_representations(p_cluster_text, model_name, mean)\n",
    "        q_cluster_representations = get_representations(q_cluster_text, model_name, mean)\n",
    "        clustering_model = train_clusters(p_cluster_representations, q_cluster_representations, \n",
    "                                          params['num_buckets'], params['kmeans_explained_var'], 0, norm='l2')\n",
    "        _, labels = get_clusters(clustering_model['pca'], clustering_model['kmeans'].index,\n",
    "                                 clustering_model['dimensionality'], test_representations)\n",
    "        _, labels_clusters = get_clusters(clustering_model['pca'], clustering_model['kmeans'].index,\n",
    "                                          clustering_model['dimensionality'], train_representations)\n",
    "\n",
    "        return  list(labels_clusters), list(labels)\n",
    "    \n",
    "    keys = args.keys()\n",
    "    vals = args.values()\n",
    "    data = []\n",
    "    base_counts = Counter(train_labels)\n",
    "    default_class = max(dict(base_counts),key=dict(base_counts).get)\n",
    "    base_acc = base_counts[default_class]/len(train_labels)\n",
    "    all_combs = list(itertools.product(*vals))\n",
    "    for comb in all_combs:\n",
    "        temp = {k:v for k,v in zip(keys,comb)}\n",
    "        train_preds, test_preds = func(temp)\n",
    "        assert len(test_preds) == len(test_labels)\n",
    "        num_buckets = temp['num_buckets'] if 'num_buckets' in temp else default_args['num_buckets']\n",
    "\n",
    "        def counts_per_class(preds, labels, num_classes):\n",
    "            classes = [[] for i in range(num_classes)]\n",
    "            for i,j  in enumerate(preds):\n",
    "                classes[j-1].append(labels[i])\n",
    "            return classes\n",
    "        train_classes = counts_per_class(train_preds, train_labels, num_buckets)\n",
    "        test_classes = counts_per_class(test_preds, test_labels, num_buckets)\n",
    "        print(num_buckets, len(np.unique(test_preds)))\n",
    "        train_counts = [Counter(i) for i in train_classes]\n",
    "        test_counts = [Counter(i) for i in test_classes]\n",
    "        acc = sum([test_counts[i].get(max(dict(c),key=dict(c).get) if c else default_class,0) for i, c in enumerate(train_counts)])/len(test_labels)\n",
    "        temp['cluster_text'] = hash(temp['cluster_text'])\n",
    "        temp['acc'] = acc\n",
    "        temp['base'] = base_acc\n",
    "        temp['pred'] = test_preds\n",
    "        temp['classes'] = test_classes\n",
    "        data.append(temp)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "acc_df = param_search(args, default_args, trn_labels, tst_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11227235",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df_r = acc_df.drop(['pred','classes'], axis=1)\n",
    "acc_df_r['cluster_text'] = acc_df_r['cluster_text'].astype(str)\n",
    "acc_df_r.loc[acc_df['cluster_text'] == hash((tuple(p_text), tuple(q_text))), 'cluster_text'] = \"WebText\" \n",
    "%R -i acc_df_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0df991",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "supp.labs <- c(\"GPT Small\",  \"GPT Medium\", \"GPT Large\",\"GPT XL\", 'BERT', 'BERT Large')\n",
    "names(supp.labs) <- c(\"gpt2\",  \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", \"bert-base-cased\", 'bert-large-cased')\n",
    "\n",
    "#data.names <- c(\"WebText\", \"Training Set\")\n",
    "ggplot(aes(x = num_buckets, y = acc, \n",
    "           color=factor(featurize_model_name, levels=names(supp.labs), labels=supp.labs),\n",
    "           shape=factor(featurize_model_name, levels=names(supp.labs), labels=supp.labs),\n",
    "          ), \n",
    "       data = filter(acc_df_r, featurize_model_name %in% names(supp.labs))) +\n",
    "    geom_point(size=5) +\n",
    "    geom_line() +\n",
    "    geom_hline(aes(yintercept = base), linetype=\"longdash\") +\n",
    "    facet_wrap(is_mean~factor(cluster_text))+ \n",
    "    scale_x_continuous(trans='log2', breaks=c(2,5,10,50,25,100,250,500)) +\n",
    "    scale_shape_manual(values = c(15,16,17,18, 19, 20)) +\n",
    "    theme_bw() +\n",
    "    labs(x = \"Number of Clusters\", y=\"Accuracy\", shape=\"GPT-2\", color=\"GPT-2\", linetype=\"Cluster Text\") +\n",
    "    theme(text=element_text(size=22,family=\"serif\"), \n",
    "         axis.text.y=element_text(size=19,family=\"serif\"),\n",
    "        axis.text.x=element_text(size=13,family=\"serif\", angle=30),\n",
    "         aspect.ratio=1.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b06f0",
   "metadata": {},
   "source": [
    "## Surface Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf9e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.load_gpt2_dataset import load_gpt2_dataset\n",
    "p_text = load_gpt2_dataset('data/amazon.valid.jsonl', num_examples=num) \n",
    "q_text = load_gpt2_dataset('data/amazon-xl-1542M.valid.jsonl', num_examples=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662bb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text1, test_text2 =  p_text2, q_text2\n",
    "train_text1, train_text2 = p_text, q_text\n",
    "trn_stop_y = [sum([1 if w in stops else 0 for w in t.lower().split()])/len(t.split()) for t in  p_text + q_text]\n",
    "tst_stop_y = [sum([1 if w in stops else 0 for w in t.lower().split()])/len(t.split()) for t in  p_text2 + q_text2]\n",
    "\n",
    "trn_punct_y = [sum([1 if not w.isalpha() else 0 for w in t.split()])/len(t.split()) for t in  p_text + q_text]\n",
    "tst_punct_y = [sum([1 if not w.isalpha() else 0 for w in t.split()])/len(t.split()) for t in  p_text2 + q_text2]\n",
    "\n",
    "default_args = {'test_text': (tuple(test_text1), tuple(test_text2)), 'cluster_text': (tuple(p_text), tuple(q_text)), \n",
    "                'featurize_model_name':'gpt2','kmeans_num_redo':5, \n",
    "                'kmeans_max_iter':100, 'kmeans_explained_var':0.9, 'is_mean':False, \n",
    "                'device_id':0, 'max_text_length': MAX_LEN, 'verbose':True}\n",
    "args = {'featurize_model_name':[\"bert-base-cased\", 'bert-large-cased','gpt2','gpt2-medium','gpt2-large'],\n",
    "        'kmeans_explained_var':[0.9], \n",
    "        'is_mean': [True, False],\n",
    "        'num_buckets':[5, 10, 20, 50, 100, 250, 500]}\n",
    "\n",
    "\n",
    "def param_search(args, default_args, train_ys, test_ys):\n",
    "    def func(cur_args):\n",
    "        params = copy.deepcopy(default_args)\n",
    "        params.update(cur_args)\n",
    "        p_text, q_text = params['test_text']\n",
    "        p_cluster_text, q_cluster_text = params['cluster_text']\n",
    "        model_name, mean = params['featurize_model_name'], params['is_mean']\n",
    "        \n",
    "        p_representations = get_representations(p_text, model_name, mean)\n",
    "        q_representations = get_representations(q_text, model_name, mean)\n",
    "        p_cluster_representations = get_representations(p_cluster_text, model_name, mean)\n",
    "        q_cluster_representations = get_representations(q_cluster_text, model_name, mean)\n",
    "        clustering_model = train_clusters(p_cluster_representations, q_cluster_representations, \n",
    "                                          params['num_buckets'], params['kmeans_explained_var'], 0, norm='l2')\n",
    "        _, labels = get_clusters(clustering_model['pca'], clustering_model['kmeans'].index,\n",
    "                                 clustering_model['dimensionality'], \n",
    "                                 np.concatenate([p_representations, q_representations], axis=0))\n",
    "        _, labels_clusters = get_clusters(clustering_model['pca'], clustering_model['kmeans'].index,\n",
    "                                          clustering_model['dimensionality'], \n",
    "                                          np.concatenate([p_cluster_representations, q_cluster_representations], axis=0))\n",
    "\n",
    "        return  list(labels_clusters), list(labels)\n",
    "    \n",
    "    keys = args.keys()\n",
    "    vals = args.values()\n",
    "    data = []\n",
    "    y_bar = np.mean(train_ys)\n",
    "    sst = sum((test_ys - y_bar)**2)\n",
    "    for comb in list(itertools.product(*vals)):\n",
    "        temp = {k:v for k,v in zip(keys,comb)}\n",
    "        train_preds, test_preds = func(temp)\n",
    "        assert len(test_preds) == len(test_ys)\n",
    "        num_buckets = temp.get('num_buckets', default_args.get('num_buckets',0))\n",
    "        def vals_per_class(preds, ys, num_classes):\n",
    "            classes = [[] for i in range(num_classes)]\n",
    "            for i,j  in enumerate(preds):\n",
    "                classes[j-1].append(ys[i])\n",
    "            return classes\n",
    "        \n",
    "        train_classes = vals_per_class(train_preds, train_ys, num_buckets)      \n",
    "        test_classes = vals_per_class(test_preds, test_ys, num_buckets)\n",
    "        train_coefs = [np.mean(c) for c in train_classes]\n",
    "        sse = sum([sum([(y - train_coefs[i])**2 for y in ys]) for i, ys in enumerate(test_classes)])\n",
    "        temp['cluster_text'] = hash(temp.get('cluster_text', default_args.get('cluster_text')))\n",
    "        temp['R^2'] = 1 - sse/sst\n",
    "        temp['pred'] = test_preds\n",
    "        temp['classes'] = test_classes\n",
    "        data.append(temp)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "punct_r2_df = param_search(args, default_args, trn_punct_y, tst_punct_y)\n",
    "stop_r2_df = param_search(args, default_args, trn_stop_y, tst_stop_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_df = pd.concat([punct_r2_df.assign(feature='punct'), stop_r2_df.assign(feature='stop')])\n",
    "r2_df_r = r2_df.drop(['pred','classes'], axis=1)\n",
    "r2_df_r['cluster_text'] = r2_df['cluster_text'].astype(str)\n",
    "%R -i r2_df_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd9836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "model_levels <- c(\"gpt2\",  \"gpt2-medium\", \"gpt2-large\",  \"bert-base-cased\", 'bert-large-cased')\n",
    "\n",
    "# data.names <- c(\"WebText\", \"Training Set\")\n",
    "ggplot(aes(x = num_buckets, y = `R^2`, \n",
    "           color=factor(featurize_model_name, levels=model_levels),\n",
    "           shape=factor(featurize_model_name, levels=model_levels)), data = r2_df_r) +\n",
    "    geom_point(size=4) +\n",
    "    geom_line() +\n",
    "    facet_wrap(is_mean~factor(feature, levels=c('punct', 'stop'), labels=c('Punctuation', 'Stopwords')))+\n",
    "    scale_x_continuous(trans='log2', breaks=c(2,5,10,50,20,100,250,500)) +\n",
    "    theme_bw() +\n",
    "    geom_hline(aes(yintercept = 0), linetype=2) +\n",
    "    labs(x = \"Number of Clusters\", y=expression(paste(R^2))) +\n",
    "    scale_color_manual(values=c(\"#F8766D\", \"#C49A00\", \"#A58AFF\", \"#00C094\", 'green', 'orange'), labels=c(\"Small\", \"Medium\", \"Large\",  \"Base\", \"Large2\"), name=\"GPT-2\") +\n",
    "    scale_shape_manual(values = c(15,16,17,18, 19, 20), labels=c(\"Small\", \"Medium\", \"Large\",  \"Base\", \"Large2\"), name=\"GPT-2\") +\n",
    "    theme(text=element_text(size=22,family=\"serif\"), \n",
    "         axis.text.y=element_text(size=17,family=\"serif\"),\n",
    "         legend.title=element_text(size=20,family=\"serif\"),\n",
    "        axis.text.x=element_text(size=17,family=\"serif\", angle=30),\n",
    "         aspect.ratio=1.25)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
